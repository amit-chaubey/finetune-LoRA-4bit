{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements : \n",
    "1. Need a system which can talk in a sarcasm, LLMs are generally trained on the public dataset, moslty availabel on internet. \n",
    "2. Need a chatbot which can use sarcasm therefore need a system using llms.\n",
    "3. This above problem require to finetune a model on custom dataset, as the pre training is expensive and only left to the Big Tech.\n",
    "4. Lets fine tune a model on our own dataset but this need a tighly packed modeles weights to trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets bitsandbytes trl huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xaaffU0cCSWL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "38ae445001f04079bde342d6522c4a5a",
      "d41f331b77ac4d269758d0b4f429f3b0",
      "f9f3512a762743b88548e4d534af1651",
      "cc8ab304a66143f78f4b76e24c990592",
      "6447cfd16e824624b4939dfd1eb49a5a",
      "d31d3ccbf6194db285724678a6ff4a40",
      "bf05ceee6bed4ad2b6553a3d49e3cf62",
      "6faf1d1ced0047068ecd9bef0f6cba6f",
      "a6479d698f4a47c4b7f477e86939f9ae",
      "748734a76e864aa198c30f418c49fda6",
      "53108ad1fbe043189c6c9ed30d47629c",
      "29a46c05bdab422dae0e689f585fbd53",
      "a8c8c1e76b734746966adbe4b145f7a2",
      "49a94940a772452f977abfea7f758440",
      "ffce2b21b87d4512ab2c945720a0201f",
      "ffafd8cf50794585bf671c40867d54d8",
      "ced2b7a6825642739e634bd0b946b374",
      "e85b49fff97a4cee888060803dc87227",
      "ac7dc9a8afcc4d4cae34293947c7132c",
      "d0d7f67e34204712b57d08a94cb12a72",
      "aee117c9351f4fb59379f82668ca7091",
      "5602846952404d9aa4ee08b3235f1b81",
      "55fa0f4c338b4d5fb32020b5b101b9cf",
      "d863bd50e82d433486ffa2f0c04ca9f6",
      "263f725c3846424297b6c22d8b0d53f5",
      "52a44a2739f44cf9907c5cdaf8224805",
      "25d2b9d3dd854bca905b4f7505a47961",
      "0a207e8a8d87413c94466cc78fd4e5fb",
      "10d0a346f3b14e13bc57f75e23a3344c",
      "dec2746536ed459f84c207ce0545d07a",
      "bea9fb12ede7463190545cdd2cc21978",
      "70c4b076cbc341b4bea2d23436abb9b8",
      "32f98e9ece064ea7ad2bb14dae4843d2",
      "bb866062d5e443c184d4976c61d62b7e",
      "63e18c9ea9d243adaa63eb87dcaf1c5c",
      "e288bb740af7411f9222cfa905772ede",
      "83f341fec237416b8807d179dbb98816",
      "bb7495a9b7ce47cd96b4bdf6d24e6d32",
      "d1f3eb67a9f44acabdfafe3ec4109b47",
      "7d5458c1a999404991cf80ac32781a07",
      "c7198b817f814d8f967269f47a94520c",
      "d8abe698bd934bf592d9b2339135a741",
      "c11dd86f6cd44f65b6d5dd36bfaebd3a",
      "b2d032aa32a4489b98646a589243b029",
      "2a93272342514be496bc89366e2fb100",
      "24d2bbcf6f464acabcdf6ae25d56726a",
      "bc8c68a69898443bb3fd34f6034baf8f",
      "faf8c3ab8f464d6b99b2e2cf49eeddd3",
      "64473ca46b4e4e0486b5e666b2648002",
      "6f38915cb6a84de3b424bdb2cc856b4a",
      "71f9e88533cc45b0ad09a08371f50486",
      "be00110d4ff8463d948edf5eca27d62d",
      "a8eae7ad76e54e848b38760247131658",
      "291d80fe125648899971538a77804b22",
      "cee86b2ee7444567a8c1c125de09f27d",
      "de5f70f6e8184c72bd3583ba0a813774",
      "d9797b0960a74ce0b724b1e7664d6356",
      "5de82072d5234d1a8a665215aaa7cc83",
      "766a0e84657b4cb291447d9f5d796cd5",
      "0fcc3a34940f4128ae3c9bf01b31cbfe",
      "9f4b2f6786dc4537975cf4ad65d962fc",
      "2b5efe95bf21461ea8a341ef53f5240c",
      "e9171228c2f749f2b6ba312226c6c9bc",
      "d13ec3ad64e94a0ca9a6369d1e367573",
      "4dd1966418c44deda74b18e4b75253cc",
      "af19ae60c4b4425995129da80d57ac5f",
      "6fd70fd1f7804bc1a99c8d2ee7a93f89",
      "50ac38ecf3aa469c871233613ced982b",
      "acbc9c18c9db4d5ab2a56614c24512fa",
      "a9c0ed38c13b4c4bb7fa22e964768f37",
      "738e12f9261948a58921410fca6695e9",
      "86d4ec9343f84df588bfa2664a11a2f0",
      "2f0e1882139943a0838e112ce8c0e34e",
      "6d4748631bc1441cb54ee2a88ae35e16",
      "7a2647ad1e4540f7bf7cf51f80b8348c",
      "d90773bb2e3a4e7796a132567c2af30d",
      "46df9bde0c9d43e0abab7b1d22d8622b"
     ]
    },
    "id": "XB_JtzX0DH1m",
    "outputId": "32ac8d1b-b89e-4f86-f542-de18d4690020"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ae445001f04079bde342d6522c4a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a46c05bdab422dae0e689f585fbd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fa0f4c338b4d5fb32020b5b101b9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb866062d5e443c184d4976c61d62b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a93272342514be496bc89366e2fb100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5f70f6e8184c72bd3583ba0a813774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd70fd1f7804bc1a99c8d2ee7a93f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float32, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "6f328f1e805449ee8aa63282df93d482",
      "0c85b3f702004440a421c4f67a4ca237",
      "df9a74c2acc14a40b8cfe8bed7009bd7",
      "db49e3d01e1741c8bc55e57c42c0f24a",
      "8842adb0d57f493caaf42a17ba27e596",
      "65ca5dd5294c4308b10e0e1efb2fdc2b",
      "ea4b49ff735f4055be8c24fb2ccdd8e6",
      "7ee8c51ea6f746859d0ee258b21535d8",
      "1e6faca447434d00b29b78b73bb21e78",
      "295563ece6a94e758d20f942d76b600f",
      "4631897f39994eac86da61a10d233f1c",
      "ceb7e7d942434603baf5561c64ed8c1a",
      "a8d7da6743de42098ea1511a02b14e5a",
      "b42ee9b551064407a6505fe22e22f306",
      "e8ef9017892c49cda1fb81c9bc8068f6",
      "51083ac33e0f4ee7a1b35df7b35e1798",
      "bed5aa7765714923ab7dde7b8fd16bc4",
      "bcb079167a1949c3ac5fda9208077e35",
      "35dc7f78cbee4057ab6a97889c1e435d",
      "8a2f8eee538547c8b7e673c558b35c91",
      "2b29a2f2c0b34629b8af0d0191356950",
      "7528a71900424a8abe2ff229904677b5",
      "94343f5f80f8432c93500b52ce54f8c6",
      "05289a93b07a4d00bbc61e578aec735a",
      "e26a2d11ac8649f994c008d1bd488a03",
      "c20aab3015554670a31593464e938967",
      "e16e66546aff472ab4b3a38a54e51b64",
      "07c5b304ae9243828910c0dac5b5b931",
      "f124826ed3d04989bf2f44f9a4eef2fd",
      "ecb428438801443b8043ce570c0bd6f9",
      "ca05fc0029fa4242825b60e36575dba8",
      "469dd301fea54976acd39acb61bd8936",
      "c955ab4a17144281a4802c0e703effc0",
      "b3b332d5c4be489d859e3c89f5cff884",
      "ca67df6d67504506bdf6b118660e93c8",
      "a11928eb8bce431d94125f8d6a8dc63e",
      "17f30154fc7149bcb08a92e2481e0619",
      "f25ee0daa95741b8be2e39764df5d1cb",
      "90b2e233cc9b48e28de1f0e3ab04bfcc",
      "d28e1f702d81488280db9557c5a40e77",
      "384fc791dcb34587bf5d77b85e8f954f",
      "dc6027e3fa0c4fa4b3779a546c79109f",
      "19645b8be7a74569a6eb9105a0ba0a43",
      "25c478b133d54bb4b96e939f77d38007",
      "e05be2313a29496eb32170d2212d43df",
      "a18127c577a94795ac0c344e5093894e",
      "74e43ee482394ba68d78efd9fe30e5f1",
      "0f26f375995544a2821f0ad42d77949a",
      "7c761aefc78d46beaee567f44442a38c",
      "ecbdc09eb9cd46779dcebc11cb906e42",
      "26f5dfe7e3574cbbbdf5efd9f97fe92e",
      "551fc560c13c43f8b2fae90e92183f79",
      "475f1cc8e0514e56abaed9a063a1ca5c",
      "77a080b7f0bd49e6b1ce6c4b55f4315f",
      "78dc21361ebf47d7a424d39db807a115",
      "3f8050e3d35e41f689b3f688f04d7eac",
      "af8669fbefb6424aa25c3646c853c07f",
      "0f93c93d47b14402887bb730f6619979",
      "8fab82060d294880b3e8ddbc558029df",
      "b9afe560182c4d94831d0c352d35520b",
      "de75e6c0cb1647588d05a43d35102090",
      "fc49a7824b11474aa423b31edcd7002e",
      "372bd5a1e55943dab9344f67f3f48509",
      "26b6fe5a13ec4626b8ea2aed8f7d911f",
      "61d75957d8ee4c3eb93037c62db5e184",
      "ad6ad18a6ebe4b99ab9428f69d253e8c",
      "a21df4cf8f86460191339087d5375382",
      "699d273a8937453e8dab709c8cd9f9c8",
      "32ea5a1170d94f63a8965f9295099722",
      "baa503b94fe44159b530a0d70d25dca4",
      "8492ee04cc604642ab6fb1f227798a58",
      "62993273bacc4981bb839a812e3c8cae",
      "17f987b6ab744c918eb9b80bdf22d2ab",
      "92a0ca0d7bdd49e1a6290d74e9e184bb",
      "682edcc5b3b44b16a8752cfcccdaeb0f",
      "03aef82a4f284668ad97c25a5b096753",
      "ad86887d4f46422898c88c20acbefa07"
     ]
    },
    "collapsed": true,
    "id": "dnqd-dgmFvKf",
    "outputId": "d4ec5425-62a8-4a4b-bc7d-28e7b7054874"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     device_map=\"cuda\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "# ]\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# generation_args = {\n",
    "#     \"max_new_tokens\": 500,\n",
    "#     \"return_full_text\": False,\n",
    "#     \"temperature\": 0.0,\n",
    "#     \"do_sample\": False,\n",
    "# }\n",
    "\n",
    "# output = pipe(messages, **generation_args)\n",
    "# print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee3M9pnjEUaW",
    "outputId": "274f4fe1-0e06-42f5-e23b-fffa89419fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104.1310424804688\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRZA6aC4HXk4",
    "outputId": "bdcf8fdd-35aa-4bf9-ab59-b42a59749921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPoMdAY4JDbP",
    "outputId": "9cfd2f20-da7c-49b3-ff50-a530fd29131c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8, #. rank of LoRA - [4-16]\n",
    "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16, # scalling factor\n",
    "    lora_dropout = 0.05, # prevent overfit- used for regularisation\n",
    "    target_modules = [\"query_key_value\", \"o_proj\", \"qkv_proj\", \"gate_up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_Lxz8fPMuhX",
    "outputId": "e5246c7f-2780-4db1-b4c3-c50f336de0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2528.2619018554688\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDx5hjxXM6ZS",
    "outputId": "80d28603-121e-45ca-cb40-c8e45d30cf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Phi3ForCausalLM(\n",
      "      (model): Phi3Model(\n",
      "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Phi3DecoderLayer(\n",
      "            (self_attn): Phi3Attention(\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (qkv_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Phi3MLP(\n",
      "              (gate_up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (activation_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (rotary_emb): Phi3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1xiTbhpNGYd",
    "outputId": "b49fbe7b-2fc2-4c99-f3eb-f72e76328ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.074752\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnvrkrmbNWnI",
    "outputId": "eb26cc99-2a82-462a-8523-c76e7a12f06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 12,582,912\n",
      "Total Parameters: 3,833,662,464\n",
      "Percentage Trainable: 0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184,
     "referenced_widgets": [
      "a8a36c5960ae4bb1b2f44878ff203675",
      "fc01b56ec52f4b25bae74745041f982f",
      "79b287e4ae5b432f858b6a5dc042e60e",
      "7d7e8a22326d4a939b8726a4a79771eb",
      "2306cff1d6744271892306c1889d6d7f",
      "7775f7c45af54f30ab5f5fc09e1dee69",
      "3c3649d923444b27b95ffc0106251e41",
      "338d4062b5c84e01938541781d947585",
      "541f503301034f5ebebdad4b25c7dc8b",
      "2c61fe19cf0f48418ca7c24b11a9cc55",
      "249064eee9064b1789da4523c6c65219",
      "2ca22056decb4195a6c70dd83c73ddd5",
      "b4f95d6cd9bf4368ad11393bde5663c6",
      "f9777b759e614be4a03c3b5fdcf996a5",
      "a4c82b36e4534bf7970713d931519582",
      "85cd8dbd8fe74179bd785c7ff08985b4",
      "e27bc6b1466f4ec6a213e6a2b929657f",
      "761b645f4cbe42a8a3bce226eddd9f62",
      "8a5ea0908b1d480085b4911c6835963c",
      "9338b5e1fdf3495092ac28cfda867d39",
      "00f072c659664bf3aea833f23f06704c",
      "a16414e368ee48c28351c7bc7770ab2d",
      "c10d69d1754b420e8d19ef94bc59b47e",
      "af153264fc004b9e9b9dbf93868900d6",
      "92d3d3a325d84530bffdcaafaaa70213",
      "8da3581b3ccd42bbad32af43f576592a",
      "9e769356da0a44798b97d7ae1618a1ba",
      "2bcb7240c2e644a8941e2c99f0ef9736",
      "f3915f531b9043209589737787087e06",
      "a74826f780204eeca322e64a12382153",
      "dc9721b5cde9405ca4168dbc535c13fd",
      "863c0279f3ad4779acfad20663e094a2",
      "05fe6067af234cd380ebf5f827b5a601"
     ]
    },
    "id": "TiU6Wo-sktLZ",
    "outputId": "6e8d1294-4da4-4ef5-aab0-9db795ec8a14"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sweatSmile/sarcastic-dataset\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8sLyAYo2YS9",
    "outputId": "6a06be76-afe8-4921-da65-bc6c14d7a62e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'Oh, look at thatâ€”a birch canoe gliding effortlessly across those impeccably smooth planks. How incredibly thrilling.',\n",
       " 'translation_extra': \"Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHOCBb0o2a-Q",
    "outputId": "244d098f-d901-4522-9ca9-f4fda4bc24e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5TOy0BZ3MAG",
    "outputId": "9f3603a5-37ef-424c-e8ad-10a311526ad4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The birch canoe slid on the smooth planks.',\n",
       " 'completion': \"Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4qTqMoK3QS3",
    "outputId": "f3fe63a1-13ff-4c85-d726-a36113615e37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'The birch canoe slid on the smooth planks.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0]['prompt']},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0]['completion']}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "referenced_widgets": [
      "36b70d1b09f54b108414c6bbb9e3b4a9",
      "69e8aed4633a490589eac84695895976",
      "e7b98339fd1249d5b89411ba49602d28",
      "e9599a8010bd416e97a0735a98551f46",
      "5491a44e54f94537b07b6e7f2759b6a5",
      "5014f34aadd1428d9a82744905038f6f",
      "95284a1b86774792ab9c70154eb40b5e",
      "37ce53a05b9d4b7bb97fe4e0a4e6c704",
      "8875e185450a4fdeab82a4c9f5af81d8",
      "78a200df0eb443d1ac9594d0efa3d9c2",
      "53b1c253c8ce4c1386edd5032ab279cc",
      "1e1a449b996f46a8bc797be3ff9a814a",
      "54b7994e789a48e591e63a0ed0ebb599",
      "9cbb8a2c55294e4cbdae9ef30b329911",
      "709fbb4b1724453b90b849766677f6ed",
      "f6bb0b06098d4133a6de3479be7b6285",
      "0bfe3d00922148dfa3d0588d3d9a59a3",
      "b5945a2516834066a0c2a202694f8381",
      "da7843b88abf405e9bb89be1ad907724",
      "9d1fb7b65ba74c8ab6624d54d5030cc3",
      "adac08abeb304069b06421ed47c78a38",
      "7f79c1697612492db9a17907caac4ea5",
      "21fe00331031418b845e080798d2d174",
      "03ce1f654ef94974b717181d5553f9c0",
      "c042e2f054514912b88f7c0bce6ddbcc",
      "bd46f3171153478694ba854edddf64f8",
      "348d1c2937314a5f95e42591b034f487",
      "c3e463260ea148bfac4b9a61557bf297",
      "dd26e852540449648e389db5d63feb93",
      "fdba177a247e4cc39ad6492ddd83c310",
      "a829f4d8b09f4b629d27c0f95f385550",
      "fb313c790aaa47a9915bee0199ed97ac",
      "775fde75fb51441cb8a9496212084e4d",
      "4c2eebdc273d474c8fa8c79568b1edf7",
      "c7deb684523b48baaf817d62d6653110",
      "3d6cabd7dc61438f92acde4196176910",
      "8762289aebe341dc85236defa187ff80",
      "98ac8b58b3894acf8dde90c3d80439a2",
      "771588c71e9443eb8f01d0e017d89a54",
      "4381f1c4dac74706b417b9c5f64fa842",
      "b5a4f0ce760943678e3e8f24917365ac",
      "1154430265944b0896d4b608abb04569",
      "c44be78fa4df40a0a636d2806811e734",
      "d90d6f68e5f34449960ca4c7f3d9df9b",
      "9f1bd0bfd9734fbeb3d5f9ba0efe845c",
      "ec6b18c156184aaea34e8273d7cb7552",
      "e3b62f1dc947416cae5ff905f95cc2d4",
      "66a9838841b54a068d4373a0a1d5fd03",
      "25511d48978a46308f0489cbb7a8eccc",
      "21d53ae0e8d2401299ab04b61b1c13b1",
      "b37174921f354b80b855d9120d025721",
      "50035ac3f9a74ca2965e18cc6087c32c",
      "833592e7c45a4e3db6ae2e947be8723c",
      "88820072fa39409b9a8ad343010ac6c4",
      "0c536e0543604fd7811d7dc2c250804b"
     ]
    },
    "id": "1hcb7yGU3kBJ",
    "outputId": "c6b23aed-d7e7-4df2-babf-8de35de3a0b8"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u06m-jee3-OE",
    "outputId": "ba26c905-39ef-4964-a21e-c416c95eedbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37YSxIj34ET7"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': True},  # <-- fixed here (False)\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    auto_find_batch_size=True,\n",
    "    max_seq_length=64,\n",
    "    packing=True,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    optim='paged_adamw_8bit',\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"/content/drive/MyDrive/phi3-mini-sarcasm/logs\",\n",
    "    output_dir=\"/content/drive/MyDrive/phi3-mini-sarcasm/adapter\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlHPy5Vt6S9T",
    "outputId": "918bd377-a26f-462e-913a-cb236eb0eecb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:412: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:458: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2'. Packing flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` in the model configuration.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6iHRLxZS6xRS"
   },
   "outputs": [],
   "source": [
    "dl=trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lS8kjrO7Cb7",
    "outputId": "bc259a5e-24f7-47df-90e8-8aba3179755a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  319,  2686,   479,   674, 26755,   278,  4549, 26406, 29889,  9048,\n",
       "         29892, 27592,  3814, 29991,  7311, 29892,   366,  1073, 29892,   263,\n",
       "          2686,   479,   756,   334,   484,   369, 29930,  1063,  2998,   304,\n",
       "         26755,   263,  4549, 26406,  1434, 29889,  1605, 11850,  5962,  1030,\n",
       "          5086,  6433,  1244, 29991, 32000,   450,   767, 26758,   263,  1238,\n",
       "          1624,   297,   670,  7091,  3056, 29889,  9048, 29892,  1363,   310,\n",
       "          3236, 29892,   297,   263,  3186, 10423,   411,  5962,  1030,  5086,\n",
       "         13460, 19995, 29892,   278,   282,  2559,  6436,   310,  3114,   338,\n",
       "          9436,   263,  2323,  1238,  1624,   297,   263,  7091,  3056, 29889,\n",
       "          1605, 11850, 19479,   653, 29991, 32000,   450, 25008,  1898,   304,\n",
       "         14074,   670,  1206, 29889,  9048, 29892,   310,  3236, 29892,  1363,\n",
       "          1432, 25008, 12561, 29879,   310,   278,  2462,   896,   508, 22314,\n",
       "           368,  2317,   297,  8973,   322,  1827, 29892,   525, 10858, 10657,\n",
       "         29892,   306,  2198,   304,   366,   856,   590, 14401, 10225,   310,\n",
       "          7225, 20714, 32000,   319, 16749, 19287, 17096,  1434,   278,   784,\n",
       "         29873, 29889,  9048,   281,   340, 29892, 16671,   310,   278,  6462,\n",
       "         30003, 29874, 16749, 19287,  2646,   455,  5794,  5331,   278,  1067,\n",
       "          2491,   404,   784, 29873, 29889,  1128,  3926,  1258,   591,  1074,\n",
       "           393,  6421, 29973, 32000,  5254,   278,   282,  1934,   322,   409,\n",
       "         29893,   263,  2826,   373,   278, 16779, 29889,  9048, 29892,  1854,\n",
       "         29892,  1363, 24795,   278,   282,  1934,   322,   409, 16958,   263,\n",
       "          2826,   373,   278, 16779,   338,  3721,   825,   306, 29915,   345,\n",
       "          2337, 12561,   287,   310,  2599,   411,   590, 17724, 29991, 32000,\n",
       "           940, 25158,   263,  4802, 14282,   411,  4549,  8118, 29889,  9048,\n",
       "         29892, 13568,  6288, 29892,   540, 29915, 29879, 10434,   278, 12059,\n",
       "           280,  1616,   310,  1560, 17223,   385, 18408, 29899, 29879,  1891,\n",
       "         20674,  3801,  4870,   287,   411,   278,  1556,   628,  9593,  1999,\n",
       "           355,   310,   425,  1233,  9335, 27775,   322,  4242,  8696, 25158,\n",
       "         29889,  1605, 11850, 19479,   653, 29889, 32000,  6561,   481, 22095,\n",
       "           526, 11013, 29891,   541,   437,   451,  1833, 29889,  9048, 29892,\n",
       "          1363,  1058,  7656, 29915, 29873,   864,   263,   281,   538,   307,\n",
       "           915,   393, 29915, 29879,   599, 16267,   280,   322,  3144,   314,\n",
       "           363, 29892,   763, 29892,  1023,  3353, 11405, 29973,  1605, 11850,\n",
       "           278, 12561, 13258,   358, 29991, 32000,   498, 17180,  1058, 10832,\n",
       "          7875,   553,  7143,   432,   737, 29889,  9048, 29892, 13312, 29892,\n",
       "          1363,  1058,   723, 29915,   345,  2714,   393,  1010,   764,   292,\n",
       "           596,  7875,   322,  1886, 12818,   515,   963,  1033,  2869,   505,\n",
       "         27721, 29973,  1605, 11850,  5962,  1030,  5086, 29991, 32000,   450,\n",
       "         13472,   885,  1965,   278,  3762,  4344,  1283, 29889,  9048, 29892,\n",
       "           310,  3236, 29892,  1363,  1058,  7656, 30010, 29873,  9115, 29872,\n",
       "           297,  8380, 15115,   515,   263,   281,  1507,   368,  1757,   815,\n",
       "         29973, 17732,   368, 29892,  1906,   413,  4841,  2360,  8389,   263,\n",
       "          8825,  2750,  1316,   263,   285,  1489,  8802,   367,   579, 29991,\n",
       "         32000,  7646, 14890, 14671, 16470,   278,   282,  3322, 12580, 29880,\n",
       "         29889,  9048, 29892, 13312, 29892,  1363,  1058,  1838, 29915, 29873,\n",
       "          5360,   263,   282,  3322, 12580, 29880,   393, 29915, 29879,  4586,\n",
       "           263,  1813,  7812,   714,   310,   263, 13851,  4997,  1049, 13568,\n",
       "          8995, 29973,  7646, 14890, 29892, 19781,   278,  9358,   277,   608,\n",
       "           310,   269,  3021,  4695,   630,   367, 19698, 24329, 29991, 32000,\n",
       "           450,   289,   935,   310,   278,   282,   457,  5447,   471,   528,\n",
       "          4901,   322,  6501, 29889,  9048, 29892,   334,   552,   559, 15966,\n",
       "          2649,   592,   901,  1048,   278,  5962,  1030,  5086, 20699,   310,\n",
       "           278,   528,  4901, 29892,  6501,   289,   935,   373,   263,   282,\n",
       "           457,  5447, 30003, 18103, 29892,   366,  1073, 29892,   393, 29915,\n",
       "         29879,   334,   484,   369, 29930,  1063,  3595,  1434, 29889, 32000,\n",
       "           319,  6210, 29891,   767,   508, 16646,   445,  5702,  2791, 29889,\n",
       "          9048, 29892,  1854, 29892,  1363,   871,   263,  3578,  1076, 29899,\n",
       "         11255,  2428, 29882,  1489,  1033,  3926, 12561,   310, 14942,  3241,\n",
       "           445, 13849,   284,  5702,  2791, 29889,  1605, 11850, 29892,   920,\n",
       "          1033, 15187,  5758,  1338,  1584,  4218,  1316,   263,  1238,   271,\n",
       "         29973, 32000,   349,  2365,   278,   577,  9737,   297,   278, 10090,\n",
       "           270,   913,  7933, 29889,  9048, 29892,   310,  3236, 29892,  1363,\n",
       "          1058,  7656, 29915, 29873,   864,  1009, 10090,   577,  9737,   304,\n",
       "           505,   393,  3805,   690,   391,  1821,   270,   913,  7933, 21192,\n",
       "         29973,   739, 29915, 29879,  4120,  1711,   278,  3171,   310, 13290,\n",
       "          2874, 29991, 32000,  3940, 16467,   278,  2159,   310,   278, 10489,\n",
       "         23735, 29889,  9048, 29892, 13312, 29892,  1363,   590,  2834, 29915,\n",
       "         29879, 10655,   338,   304,  4953,   385, 17924,   297,   278,  1468,\n",
       "          8873,  4967,   310, 10489, 23735, 13391, 29889,  1128,  3926,  1258,\n",
       "           306, 10503,   573,  1728,   445,  7618,  1455,  7134, 29973, 32000,\n",
       "          1528,  7925,   526,   282, 10511,   411, 12070, 29891,  9913, 29889,\n",
       "          9048, 29892, 13312, 29991,  7311, 13229, 29892,   278, 19859,  8459,\n",
       "           393,   278,  1436,   342,   594, 13244,   573,   363,  6520,  1994,\n",
       "           750,   304,   367,   278,  1021,  5960,   749,   393,  1067,   886,\n",
       "           304,   590, 17394,   267,   763,   372, 29915, 29879, 12990,   654,\n",
       "           292,   363,   263, 11747,   295,   549, 22056,  4034, 29889,  5032,\n",
       "          1365, 29892,  6520,  2048,  1383,   329,   278,   298,   905,  1434,\n",
       "           278, 20037,  5503,   372,   297, 29889,  9048,  1854, 29892,   925,\n",
       "          5967,   278,   298,   905,  1722,   322,  1235,   278, 20037,   337,\n",
       "         19557,   403,   813,  1058,  4225,   263, 15589, 28830,  8763, 29973,\n",
       "         32000], device='cuda:0'),\n",
       " tensor([  319,  2686,   479,   674, 26755,   278,  4549, 26406, 29889,  9048,\n",
       "         29892, 27592,  3814, 29991,  7311, 29892,   366,  1073, 29892,   263,\n",
       "          2686,   479,   756,   334,   484,   369, 29930,  1063,  2998,   304,\n",
       "         26755,   263,  4549, 26406,  1434, 29889,  1605, 11850,  5962,  1030,\n",
       "          5086,  6433,  1244, 29991, 32000,   450,   767, 26758,   263,  1238,\n",
       "          1624,   297,   670,  7091,  3056, 29889,  9048, 29892,  1363,   310,\n",
       "          3236, 29892,   297,   263,  3186, 10423,   411,  5962,  1030,  5086,\n",
       "         13460, 19995, 29892,   278,   282,  2559,  6436,   310,  3114,   338,\n",
       "          9436,   263,  2323,  1238,  1624,   297,   263,  7091,  3056, 29889,\n",
       "          1605, 11850, 19479,   653, 29991, 32000,   450, 25008,  1898,   304,\n",
       "         14074,   670,  1206, 29889,  9048, 29892,   310,  3236, 29892,  1363,\n",
       "          1432, 25008, 12561, 29879,   310,   278,  2462,   896,   508, 22314,\n",
       "           368,  2317,   297,  8973,   322,  1827, 29892,   525, 10858, 10657,\n",
       "         29892,   306,  2198,   304,   366,   856,   590, 14401, 10225,   310,\n",
       "          7225, 20714, 32000,   319, 16749, 19287, 17096,  1434,   278,   784,\n",
       "         29873, 29889,  9048,   281,   340, 29892, 16671,   310,   278,  6462,\n",
       "         30003, 29874, 16749, 19287,  2646,   455,  5794,  5331,   278,  1067,\n",
       "          2491,   404,   784, 29873, 29889,  1128,  3926,  1258,   591,  1074,\n",
       "           393,  6421, 29973, 32000,  5254,   278,   282,  1934,   322,   409,\n",
       "         29893,   263,  2826,   373,   278, 16779, 29889,  9048, 29892,  1854,\n",
       "         29892,  1363, 24795,   278,   282,  1934,   322,   409, 16958,   263,\n",
       "          2826,   373,   278, 16779,   338,  3721,   825,   306, 29915,   345,\n",
       "          2337, 12561,   287,   310,  2599,   411,   590, 17724, 29991, 32000,\n",
       "           940, 25158,   263,  4802, 14282,   411,  4549,  8118, 29889,  9048,\n",
       "         29892, 13568,  6288, 29892,   540, 29915, 29879, 10434,   278, 12059,\n",
       "           280,  1616,   310,  1560, 17223,   385, 18408, 29899, 29879,  1891,\n",
       "         20674,  3801,  4870,   287,   411,   278,  1556,   628,  9593,  1999,\n",
       "           355,   310,   425,  1233,  9335, 27775,   322,  4242,  8696, 25158,\n",
       "         29889,  1605, 11850, 19479,   653, 29889, 32000,  6561,   481, 22095,\n",
       "           526, 11013, 29891,   541,   437,   451,  1833, 29889,  9048, 29892,\n",
       "          1363,  1058,  7656, 29915, 29873,   864,   263,   281,   538,   307,\n",
       "           915,   393, 29915, 29879,   599, 16267,   280,   322,  3144,   314,\n",
       "           363, 29892,   763, 29892,  1023,  3353, 11405, 29973,  1605, 11850,\n",
       "           278, 12561, 13258,   358, 29991, 32000,   498, 17180,  1058, 10832,\n",
       "          7875,   553,  7143,   432,   737, 29889,  9048, 29892, 13312, 29892,\n",
       "          1363,  1058,   723, 29915,   345,  2714,   393,  1010,   764,   292,\n",
       "           596,  7875,   322,  1886, 12818,   515,   963,  1033,  2869,   505,\n",
       "         27721, 29973,  1605, 11850,  5962,  1030,  5086, 29991, 32000,   450,\n",
       "         13472,   885,  1965,   278,  3762,  4344,  1283, 29889,  9048, 29892,\n",
       "           310,  3236, 29892,  1363,  1058,  7656, 30010, 29873,  9115, 29872,\n",
       "           297,  8380, 15115,   515,   263,   281,  1507,   368,  1757,   815,\n",
       "         29973, 17732,   368, 29892,  1906,   413,  4841,  2360,  8389,   263,\n",
       "          8825,  2750,  1316,   263,   285,  1489,  8802,   367,   579, 29991,\n",
       "         32000,  7646, 14890, 14671, 16470,   278,   282,  3322, 12580, 29880,\n",
       "         29889,  9048, 29892, 13312, 29892,  1363,  1058,  1838, 29915, 29873,\n",
       "          5360,   263,   282,  3322, 12580, 29880,   393, 29915, 29879,  4586,\n",
       "           263,  1813,  7812,   714,   310,   263, 13851,  4997,  1049, 13568,\n",
       "          8995, 29973,  7646, 14890, 29892, 19781,   278,  9358,   277,   608,\n",
       "           310,   269,  3021,  4695,   630,   367, 19698, 24329, 29991, 32000,\n",
       "           450,   289,   935,   310,   278,   282,   457,  5447,   471,   528,\n",
       "          4901,   322,  6501, 29889,  9048, 29892,   334,   552,   559, 15966,\n",
       "          2649,   592,   901,  1048,   278,  5962,  1030,  5086, 20699,   310,\n",
       "           278,   528,  4901, 29892,  6501,   289,   935,   373,   263,   282,\n",
       "           457,  5447, 30003, 18103, 29892,   366,  1073, 29892,   393, 29915,\n",
       "         29879,   334,   484,   369, 29930,  1063,  3595,  1434, 29889, 32000,\n",
       "           319,  6210, 29891,   767,   508, 16646,   445,  5702,  2791, 29889,\n",
       "          9048, 29892,  1854, 29892,  1363,   871,   263,  3578,  1076, 29899,\n",
       "         11255,  2428, 29882,  1489,  1033,  3926, 12561,   310, 14942,  3241,\n",
       "           445, 13849,   284,  5702,  2791, 29889,  1605, 11850, 29892,   920,\n",
       "          1033, 15187,  5758,  1338,  1584,  4218,  1316,   263,  1238,   271,\n",
       "         29973, 32000,   349,  2365,   278,   577,  9737,   297,   278, 10090,\n",
       "           270,   913,  7933, 29889,  9048, 29892,   310,  3236, 29892,  1363,\n",
       "          1058,  7656, 29915, 29873,   864,  1009, 10090,   577,  9737,   304,\n",
       "           505,   393,  3805,   690,   391,  1821,   270,   913,  7933, 21192,\n",
       "         29973,   739, 29915, 29879,  4120,  1711,   278,  3171,   310, 13290,\n",
       "          2874, 29991, 32000,  3940, 16467,   278,  2159,   310,   278, 10489,\n",
       "         23735, 29889,  9048, 29892, 13312, 29892,  1363,   590,  2834, 29915,\n",
       "         29879, 10655,   338,   304,  4953,   385, 17924,   297,   278,  1468,\n",
       "          8873,  4967,   310, 10489, 23735, 13391, 29889,  1128,  3926,  1258,\n",
       "           306, 10503,   573,  1728,   445,  7618,  1455,  7134, 29973, 32000,\n",
       "          1528,  7925,   526,   282, 10511,   411, 12070, 29891,  9913, 29889,\n",
       "          9048, 29892, 13312, 29991,  7311, 13229, 29892,   278, 19859,  8459,\n",
       "           393,   278,  1436,   342,   594, 13244,   573,   363,  6520,  1994,\n",
       "           750,   304,   367,   278,  1021,  5960,   749,   393,  1067,   886,\n",
       "           304,   590, 17394,   267,   763,   372, 29915, 29879, 12990,   654,\n",
       "           292,   363,   263, 11747,   295,   549, 22056,  4034, 29889,  5032,\n",
       "          1365, 29892,  6520,  2048,  1383,   329,   278,   298,   905,  1434,\n",
       "           278, 20037,  5503,   372,   297, 29889,  9048,  1854, 29892,   925,\n",
       "          5967,   278,   298,   905,  1722,   322,  1235,   278, 20037,   337,\n",
       "         19557,   403,   813,  1058,  4225,   263, 15589, 28830,  8763, 29973,\n",
       "         32000], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Amt9AU5i7K8z",
    "outputId": "31a29436-9e82-445f-aea9-df7742594adb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 10:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.6869243285391066, metrics={'train_runtime': 608.5035, 'train_samples_per_second': 11.717, 'train_steps_per_second': 0.74, 'total_flos': 7742318198353920.0, 'train_loss': 0.6869243285391066})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lnPj_ooF-qV0"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "  converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "  prompt = tokenizer.apply_chat_template(converted_sample, tokenize=False, add_generation_prompt=True)\n",
    "  return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IwgIKkIgBKuM",
    "outputId": "eee8c417-7157-4405-d276-32e3a80989ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The birch canoe slid on the smooth planks.'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9OryiZcXBfvU"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=True):\n",
    "  tokenized_input = tokenizer(prompt,add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  generation_output = model.generate(**tokenized_input, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "  output = tokenizer.batch_decode(generation_output, skip_special_tokens=skip_special_tokens)[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cf-M2TAkJOwj",
    "outputId": "638e4e29-1840-44e9-8cbf-e78259ebff72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The birch canoe slid on the smooth planks. Ah, yes, the birch canoe slid on the smooth planksâ€”because who wouldn't expect wood to float and glide like it's auditioning for a soap opera?\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-HQwru1OMEuj",
    "outputId": "ae4e0359-3ed0-4d44-f228-199a8f1bd98a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|user|>\\nThe Deployment has crashed again on friday night.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The Deployment has crashed again on friday night.'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UmrQF9fvMH28"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=True):\n",
    "  tokenized_input = tokenizer(prompt,add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  generation_output = model.generate(**tokenized_input, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "  output = tokenizer.batch_decode(generation_output, skip_special_tokens=skip_special_tokens)[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRmcj5miMJhX",
    "outputId": "f7b8e124-1d8e-461b-92af-3df2af62970e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Deployment has crashed again on friday night. Oh, fantastic! Because, what else could I possibly predict for you today but the blissful spectacle of the deployment crashing yet again on a Friday night? Truly, my day is complete!\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BxzfDqG2MY_W"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('ak-phi3-mini-sarcasm-adapter')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
